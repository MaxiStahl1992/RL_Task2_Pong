{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.17.0 at http://localhost:6010/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=19529): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tensorboard --logdir ./sb3_logs/A2C_PongNoFrameskip-v4_10800/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sb3_data(tensorboard_log_dir, json_path, algorithm_name, duration, color, algorithm_type):\n",
    "    \"\"\"\n",
    "    Load SB3 implementation data from TensorBoard logs.\n",
    "    Calculates the total training time across multiple event files by using the min and max timestamps.\n",
    "    Adjusts total training time to match expected duration if within a tolerance of 1 minute.\n",
    "    Adds wall times for each episode reward as Episode Times.\n",
    "    \"\"\"\n",
    "    # Define expected durations in seconds\n",
    "    duration_map = {'3h': 3 * 3600, '6h': 6 * 3600, '9h': 9 * 3600}\n",
    "    expected_duration = duration_map.get(duration, None)\n",
    "\n",
    "    event_files = glob.glob(os.path.join(tensorboard_log_dir, '**', 'events.*'), recursive=True)\n",
    "    rewards = []\n",
    "    rewards_timesteps = []\n",
    "    rewards_times = []  # To store wall times for episode rewards\n",
    "    losses = []\n",
    "    losses_timesteps = []\n",
    "    all_timestamps = []\n",
    "\n",
    "    # Load evaluation data from JSON\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process each event file to extract relevant metrics and timestamps\n",
    "    for event_file in event_files:\n",
    "        ea = event_accumulator.EventAccumulator(event_file)\n",
    "        ea.Reload()\n",
    "\n",
    "        # Rewards\n",
    "        if 'rollout/ep_rew_mean' in ea.Tags().get('scalars', []):\n",
    "            events = ea.Scalars('rollout/ep_rew_mean')\n",
    "            rewards.extend([e.value for e in events])\n",
    "            rewards_timesteps.extend([e.step for e in events])\n",
    "            rewards_times.extend([e.wall_time for e in events])  # Collect wall times for rewards\n",
    "            all_timestamps.extend([e.wall_time for e in events])\n",
    "\n",
    "        # Losses\n",
    "        if 'train/loss' in ea.Tags().get('scalars', []):\n",
    "            events = ea.Scalars('train/loss')\n",
    "            losses.extend([e.value for e in events])\n",
    "            losses_timesteps.extend([e.step for e in events])\n",
    "            all_timestamps.extend([e.wall_time for e in events])\n",
    "\n",
    "    # Calculate total training time using the earliest and latest timestamps across all files\n",
    "    if all_timestamps:\n",
    "        calculated_training_time = max(all_timestamps) - min(all_timestamps)\n",
    "        # Adjust training time to match expected duration if within tolerance\n",
    "        if expected_duration and abs(calculated_training_time - expected_duration) <= 60:\n",
    "            total_training_time = expected_duration  # Use expected duration if close enough\n",
    "        else:\n",
    "            total_training_time = calculated_training_time  # Use calculated time otherwise\n",
    "    else:\n",
    "        total_training_time = None\n",
    "\n",
    "    # Calculate average timesteps per episode\n",
    "    if len(rewards_timesteps) > 1:\n",
    "        diffs = np.diff(rewards_timesteps)\n",
    "        avg_timesteps = np.mean(diffs).tolist()\n",
    "    else:\n",
    "        avg_timesteps = []\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Algorithm Name': [algorithm_name],\n",
    "        'Duration': [duration],\n",
    "        'Type': [algorithm_type],\n",
    "        'Color': [color],\n",
    "        'Episode Rewards': [rewards],\n",
    "        'Episode Times': [rewards_times],  # Add wall times for each reward\n",
    "        'Episode Timesteps': [rewards_timesteps],\n",
    "        'Losses': [losses],\n",
    "        'Loss Times': [None],  # Keeping Loss Times empty for now\n",
    "        'Loss Timesteps': [losses_timesteps],\n",
    "        'Total Training Time': [total_training_time],  # Adjusted total training time in seconds\n",
    "        'Total Episodes': [len(rewards)],\n",
    "        'Average Timesteps per Episode': [avg_timesteps],\n",
    "        'Mean Evaluation Reward': [data.get('mean_reward', None)],\n",
    "        'Std Evaluation Reward': [data.get('std_reward', None)]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold all dataframes\n",
    "dfs = []\n",
    "\n",
    "# Specify the algorithms you want to load\n",
    "algorithms = [\n",
    "    {'type': 'sb3', 'name': 'DQN_SB3_3h', 'log_dir': './sb3_logs/DQN_PongNoFrameskip-v4_10800/', 'json_path': './metrics/sb3_eval/dqn_model_10800/metrics.json', 'duration': '3h', 'color': '#80DEEA'}, \n",
    "    {'type': 'sb3', 'name': 'A2C_SB3_3h', 'log_dir': './sb3_logs/A2C_PongNoFrameskip-v4_10800/', 'json_path': './metrics/sb3_eval/a2c_model_10800/metrics.json', 'duration': '3h', 'color': '#F8BBD0'},\n",
    "    {'type': 'sb3', 'name': 'PPO_SB3_3h', 'log_dir': './sb3_logs/PPO_PongNoFrameskip-v4_10800/', 'json_path': './metrics/sb3_eval/ppo_model_10800/metrics.json', 'duration': '3h', 'color': '#C5E1A5'},\n",
    "    {'type': 'sb3', 'name': 'DQN_SB3_6h', 'log_dir': './sb3_logs/DQN_PongNoFrameskip-v4_21600/', 'json_path': './metrics/sb3_eval/dqn_model_21600/metrics.json', 'duration': '6h', 'color': '#00ACC1'},\n",
    "    {'type': 'sb3', 'name': 'A2C_SB3_6h', 'log_dir': './sb3_logs/A2C_PongNoFrameskip-v4_21600/', 'json_path': './metrics/sb3_eval/a2c_model_21600/metrics.json', 'duration': '6h', 'color': '#EC407A'},\n",
    "    {'type': 'sb3', 'name': 'PPO_SB3_6h', 'log_dir': './sb3_logs/PPO_PongNoFrameskip-v4_21600/', 'json_path': './metrics/sb3_eval/ppo_model_21600/metrics.json', 'duration': '6h', 'color': '#7CB342'},\n",
    "    {'type': 'sb3', 'name': 'DQN_SB3_9h', 'log_dir': './sb3_logs/DQN_PongNoFrameskip-v4_32400/', 'json_path': './metrics/sb3_eval/dqn_model_32400/metrics.json', 'duration': '9h', 'color': '#00838F'},\n",
    "    {'type': 'sb3', 'name': 'A2C_SB3_9h', 'log_dir': './sb3_logs/A2C_PongNoFrameskip-v4_32400/', 'json_path': './metrics/sb3_eval/a2c_model_32400/metrics.json', 'duration': '9h', 'color': '#AD1457'},\n",
    "    {'type': 'sb3', 'name': 'PPO_SB3_9h', 'log_dir': './sb3_logs/PPO_PongNoFrameskip-v4_32400/', 'json_path': './metrics/sb3_eval/ppo_model_32400/metrics.json', 'duration': '9h', 'color': '#33691E'},\n",
    "]\n",
    "\n",
    "# Load data for each algorithm\n",
    "for algo in algorithms:\n",
    "    df = load_sb3_data(\n",
    "        tensorboard_log_dir=algo['log_dir'], \n",
    "        json_path=algo['json_path'],\n",
    "        algorithm_name=algo['name'], \n",
    "        duration=algo['duration'], \n",
    "        color=algo['color'], \n",
    "        algorithm_type='SB3'\n",
    "    )\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "data = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.17.0 at http://localhost:6008/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while terminating subprocess (pid=18683): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1026 08:03:02.026597 6147665920 plugin_event_multiplexer.py:267] Deleting accumulator '.'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
